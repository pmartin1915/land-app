name: Alabama Auction Watcher CI/CD Pipeline

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main ]
  schedule:
    # Run tests daily at 6 AM UTC
    - cron: '0 6 * * *'

env:
  PYTHON_VERSION: '3.10'
  NODE_VERSION: '18'

jobs:
  # Job 1: Code Quality and Static Analysis
  code-quality:
    name: Code Quality Analysis
    runs-on: ubuntu-latest
    timeout-minutes: 15

    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
        cache: 'pip'

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install flake8 black isort mypy bandit safety

    - name: Code formatting check (Black)
      run: black --check --diff .

    - name: Import sorting check (isort)
      run: isort --check-only --diff .

    - name: Linting (flake8)
      run: flake8 . --count --select=E9,F63,F7,F82 --show-source --statistics

    - name: Type checking (mypy)
      run: mypy scripts/ config/ --ignore-missing-imports
      continue-on-error: true

    - name: Security scanning (bandit)
      run: bandit -r scripts/ config/ -f json -o bandit-report.json
      continue-on-error: true

    - name: Dependency vulnerability check
      run: safety check --json --output safety-report.json
      continue-on-error: true

    - name: Upload security reports
      uses: actions/upload-artifact@v3
      if: always()
      with:
        name: security-reports
        path: |
          bandit-report.json
          safety-report.json

  # Job 2: Unit Tests (Fast, Parallel)
  unit-tests:
    name: Unit Tests
    runs-on: ubuntu-latest
    timeout-minutes: 20
    strategy:
      matrix:
        test-group: [parser, scraper, utils, exceptions, config]

    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
        cache: 'pip'

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install pytest-xdist pytest-cov pytest-html pytest-json-report

    - name: Run unit tests for ${{ matrix.test-group }}
      run: |
        python -m pytest tests/unit/test_${{ matrix.test-group }}.py \
          -v --tb=short \
          --cov=scripts --cov=config \
          --cov-report=xml:coverage-${{ matrix.test-group }}.xml \
          --cov-report=html:htmlcov-${{ matrix.test-group }} \
          --json-report --json-report-file=results-${{ matrix.test-group }}.json \
          --html=report-${{ matrix.test-group }}.html --self-contained-html \
          -n auto --dist=loadbalance

    - name: Upload test results
      uses: actions/upload-artifact@v3
      if: always()
      with:
        name: unit-test-results-${{ matrix.test-group }}
        path: |
          coverage-${{ matrix.test-group }}.xml
          htmlcov-${{ matrix.test-group }}/
          results-${{ matrix.test-group }}.json
          report-${{ matrix.test-group }}.html

  # Job 3: Integration Tests (Medium speed, Limited Parallel)
  integration-tests:
    name: Integration Tests
    runs-on: ubuntu-latest
    timeout-minutes: 30
    needs: unit-tests
    strategy:
      matrix:
        test-suite: [scraper-workflows, csv-processing]

    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
        cache: 'pip'

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install pytest-xdist pytest-cov pytest-html pytest-json-report pytest-timeout

    - name: Run integration tests for ${{ matrix.test-suite }}
      run: |
        python -m pytest tests/integration/test_${{ matrix.test-suite }}.py \
          -v --tb=short \
          --timeout=300 \
          --cov=scripts --cov=config \
          --cov-report=xml:coverage-integration-${{ matrix.test-suite }}.xml \
          --json-report --json-report-file=results-integration-${{ matrix.test-suite }}.json \
          --html=report-integration-${{ matrix.test-suite }}.html --self-contained-html \
          -n 2 --dist=loadbalance

    - name: Upload integration test results
      uses: actions/upload-artifact@v3
      if: always()
      with:
        name: integration-test-results-${{ matrix.test-suite }}
        path: |
          coverage-integration-${{ matrix.test-suite }}.xml
          results-integration-${{ matrix.test-suite }}.json
          report-integration-${{ matrix.test-suite }}.html

  # Job 4: End-to-End Tests (Slower, Sequential)
  e2e-tests:
    name: End-to-End Tests
    runs-on: ubuntu-latest
    timeout-minutes: 45
    needs: integration-tests

    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
        cache: 'pip'

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install pytest-xdist pytest-cov pytest-html pytest-json-report pytest-timeout

    - name: Create test data directories
      run: |
        mkdir -p data/raw data/processed logs

    - name: Run e2e tests
      run: |
        python -m pytest tests/e2e/ \
          -v --tb=short \
          --timeout=600 \
          --cov=scripts --cov=config --cov=streamlit_app \
          --cov-report=xml:coverage-e2e.xml \
          --cov-report=html:htmlcov-e2e \
          --json-report --json-report-file=results-e2e.json \
          --html=report-e2e.html --self-contained-html \
          -n 2 --dist=loadbalance

    - name: Upload e2e test results
      uses: actions/upload-artifact@v3
      if: always()
      with:
        name: e2e-test-results
        path: |
          coverage-e2e.xml
          htmlcov-e2e/
          results-e2e.json
          report-e2e.html

  # Job 5: Parallel Test Execution with AI Analysis
  parallel-test-execution:
    name: Parallel Test Execution with AI Analysis
    runs-on: ubuntu-latest
    timeout-minutes: 60
    needs: [code-quality, unit-tests]

    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
        cache: 'pip'

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install pytest-xdist pytest-cov pytest-html pytest-json-report pytest-timeout pytest-benchmark

    - name: Create required directories
      run: |
        mkdir -p data/raw data/processed logs test_reports

    - name: Run parallel test execution pipeline
      run: |
        python scripts/parallel_test_executor.py

    - name: Upload parallel execution results
      uses: actions/upload-artifact@v3
      if: always()
      with:
        name: parallel-execution-results
        path: |
          test_reports/

    - name: Upload coverage to Codecov
      uses: codecov/codecov-action@v3
      if: always()
      with:
        files: test_reports/coverage.xml
        flags: parallel-execution
        name: parallel-test-coverage

  # Job 6: Performance Benchmarks
  performance-benchmarks:
    name: Performance Benchmarks
    runs-on: ubuntu-latest
    timeout-minutes: 30
    needs: unit-tests
    if: github.event_name == 'push' || github.event_name == 'schedule'

    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
        cache: 'pip'

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install pytest-benchmark pytest-html pytest-json-report

    - name: Run performance benchmarks
      run: |
        python -m pytest tests/benchmarks/ \
          -v --benchmark-only \
          --benchmark-json=benchmark-results.json \
          --benchmark-html=benchmark-report.html \
          --benchmark-compare-fail=min:5% \
          --benchmark-sort=mean

    - name: Store benchmark results
      uses: benchmark-action/github-action-benchmark@v1
      if: github.event_name == 'push'
      with:
        tool: 'pytest'
        output-file-path: benchmark-results.json
        github-token: ${{ secrets.GITHUB_TOKEN }}
        auto-push: true

    - name: Upload benchmark results
      uses: actions/upload-artifact@v3
      if: always()
      with:
        name: performance-benchmarks
        path: |
          benchmark-results.json
          benchmark-report.html

  # Job 7: Security and Compliance Tests
  security-tests:
    name: Security and Compliance Tests
    runs-on: ubuntu-latest
    timeout-minutes: 20

    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
        cache: 'pip'

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install bandit semgrep safety

    - name: Run security tests
      run: |
        # Bandit security linting
        bandit -r scripts/ config/ streamlit_app/ -f json -o bandit-security.json

        # Semgrep static analysis
        python -m semgrep --config=auto --json --output=semgrep-results.json scripts/ config/ || true

        # Dependency vulnerability scanning
        safety check --json --output=safety-security.json || true

    - name: Upload security test results
      uses: actions/upload-artifact@v3
      if: always()
      with:
        name: security-test-results
        path: |
          bandit-security.json
          semgrep-results.json
          safety-security.json

  # Job 8: Test Results Aggregation and Reporting
  test-aggregation:
    name: Test Results Aggregation
    runs-on: ubuntu-latest
    timeout-minutes: 15
    needs: [unit-tests, integration-tests, e2e-tests, parallel-test-execution]
    if: always()

    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Download all test artifacts
      uses: actions/download-artifact@v3

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
        cache: 'pip'

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install coverage jinja2

    - name: Aggregate coverage reports
      run: |
        coverage combine --data-file=.coverage.combined || true
        coverage xml -o coverage-combined.xml || true
        coverage html -d htmlcov-combined || true
        coverage report --show-missing > coverage-report.txt || true

    - name: Generate consolidated test report
      run: |
        python -c "
        import json
        import glob
        import os
        from pathlib import Path

        # Aggregate all JSON test results
        all_results = {'suites': [], 'summary': {'total': 0, 'passed': 0, 'failed': 0, 'skipped': 0}}

        for result_file in glob.glob('**/results-*.json', recursive=True):
            try:
                with open(result_file, 'r') as f:
                    data = json.load(f)
                    if 'summary' in data:
                        summary = data['summary']
                        all_results['summary']['total'] += summary.get('total', 0)
                        all_results['summary']['passed'] += summary.get('passed', 0)
                        all_results['summary']['failed'] += summary.get('failed', 0)
                        all_results['summary']['skipped'] += summary.get('skipped', 0)
                    all_results['suites'].append({'file': result_file, 'data': data})
            except:
                pass

        # Calculate success rate
        total = all_results['summary']['total']
        passed = all_results['summary']['passed']
        all_results['summary']['success_rate'] = (passed / total * 100) if total > 0 else 0

        with open('consolidated-test-results.json', 'w') as f:
            json.dump(all_results, f, indent=2)
        "

    - name: Create test summary
      run: |
        echo '## Test Execution Summary' > test-summary.md
        echo '' >> test-summary.md
        python -c "
        import json
        with open('consolidated-test-results.json', 'r') as f:
            data = json.load(f)
        summary = data['summary']
        print(f'- **Total Tests**: {summary[\"total\"]}')
        print(f'- **Passed**: {summary[\"passed\"]}')
        print(f'- **Failed**: {summary[\"failed\"]}')
        print(f'- **Skipped**: {summary[\"skipped\"]}')
        print(f'- **Success Rate**: {summary[\"success_rate\"]:.1f}%')
        " >> test-summary.md

    - name: Upload consolidated results
      uses: actions/upload-artifact@v3
      if: always()
      with:
        name: consolidated-test-results
        path: |
          consolidated-test-results.json
          test-summary.md
          coverage-combined.xml
          htmlcov-combined/
          coverage-report.txt

    - name: Upload final coverage to Codecov
      uses: codecov/codecov-action@v3
      if: always()
      with:
        files: coverage-combined.xml
        flags: consolidated
        name: consolidated-coverage

  # Job 9: Deployment Preparation (on main branch)
  deployment-prep:
    name: Deployment Preparation
    runs-on: ubuntu-latest
    timeout-minutes: 20
    needs: [test-aggregation, security-tests]
    if: github.ref == 'refs/heads/main' && github.event_name == 'push'

    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Download consolidated test results
      uses: actions/download-artifact@v3
      with:
        name: consolidated-test-results

    - name: Check deployment readiness
      run: |
        python -c "
        import json
        import sys

        with open('consolidated-test-results.json', 'r') as f:
            data = json.load(f)

        success_rate = data['summary']['success_rate']
        failed_tests = data['summary']['failed']

        print(f'Success Rate: {success_rate:.1f}%')
        print(f'Failed Tests: {failed_tests}')

        if success_rate < 95.0:
            print('❌ Deployment blocked: Success rate below 95%')
            sys.exit(1)

        if failed_tests > 0:
            print('⚠️  Warning: Some tests failed')
            # Could still proceed with warnings

        print('✅ Deployment ready: All quality gates passed')
        "

    - name: Create deployment package
      run: |
        # Create a deployment-ready package
        mkdir -p deployment-package
        cp -r scripts/ config/ streamlit_app/ requirements.txt deployment-package/
        tar -czf alabama-auction-watcher-$(date +%Y%m%d-%H%M%S).tar.gz deployment-package/

    - name: Upload deployment package
      uses: actions/upload-artifact@v3
      with:
        name: deployment-package
        path: alabama-auction-watcher-*.tar.gz

# Global workflow configuration
concurrency:
  group: ${{ github.workflow }}-${{ github.ref }}
  cancel-in-progress: true

# Environment variables for all jobs
env:
  PYTHONUNBUFFERED: 1
  PYTHONDONTWRITEBYTECODE: 1
  PIP_NO_CACHE_DIR: 1
  PIP_DISABLE_PIP_VERSION_CHECK: 1