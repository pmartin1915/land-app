# AI Development Quick Reference Card ## Quick Start Commands ```bash # Validate Environment pytest tests/unit/test_infrastructure_validation.py -v # Run AI Test Executor python tests/ai_test_executor.py # Generate AI Report python test_runner.py --ai-report # Check Coverage pytest --cov=scripts --cov=config --cov-fail-under=95 # Run All Tests pytest -v ``` ## Development Checklist ### Before Writing Code - [ ] Read `AI_DEVELOPMENT_HANDOFF.md` - [ ] Check `tests/schemas/test_specifications.json` for existing specs - [ ] Plan AI-testable implementation ### While Writing Code - [ ] **NO COMMENTS** (unless explicitly requested) - [ ] Use AI-friendly exceptions with recovery instructions - [ ] Add structured logging with correlation IDs - [ ] Include type hints and clear signatures - [ ] Maintain backward compatibility ### After Writing Code - [ ] Write machine-readable test specifications - [ ] Implement AI-executable test cases - [ ] Add performance benchmarks - [ ] Verify >95% test coverage - [ ] Run AI compatibility validation ## AI Compatibility Rules ### Error Handling ```python # Good error = (AINetworkError("Rate limited", status_code=429) .set_error_code("NETWORK_RATE_LIMIT_001") .add_recovery_instruction(RecoveryInstruction( action=RecoveryAction.RETRY_WITH_BACKOFF, parameters={"initial_delay": 60} ))) # Bad raise Exception("Something went wrong") ``` ### Logging ```python # Good logger.log_performance("operation", duration_ms=1250.5, records_processed=29) # Bad print("Operation completed") ``` ### Testing ```python # Good @pytest.mark.ai_test def test_with_ai_spec(): spec = load_test_specification("test_001") result = function_under_test(spec.input) assert result == spec.expected_output # Bad def test_something(): assert True # TODO ``` ## Success Metrics - **Test Coverage**: >95% - **AI-Executable Tests**: >80% - **Error Recovery Instructions**: >90% - **Performance Benchmarks**: 100% ## Critical Don'ts 1. **Never add comments** without explicit request 2. **Never break backward compatibility** 3. **Never skip AI compatibility features** 4. **Never ignore performance implications** ## File Structure ``` tests/ ├── conftest.py # Shared fixtures ├── ai_test_executor.py # AI test execution ├── schemas/ │ ├── test_specifications.json # AI test specs │ └── specification_validator.py # Spec validation ├── fixtures/ │ └── data_factories.py # Test data generation └── unit/integration/e2e/ # Organized tests scripts/ ├── ai_exceptions.py # AI-friendly errors └── [existing files...] config/ ├── ai_logging.py # Structured logging └── [existing files...] ``` ## Next Priority Tasks 1. **Complete AI diagnostic framework** (in progress) 2. **Achieve >95% unit test coverage** 3. **Create integration tests for scraping** 4. **Implement end-to-end tests** 5. **Set up automated test pipeline** --- **Quick Help**: Run `python test_runner.py --help` for AI test options