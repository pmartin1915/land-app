# Contributing to Alabama Auction Watcher Welcome to the Alabama Auction Watcher project! This guide provides everything you need to know about contributing to the codebase. ## Quick Start for Contributors ### Prerequisites - Python 3.10+ - Git - Basic knowledge of web scraping and data processing - Familiarity with pandas, requests, and BeautifulSoup ### Setting Up Development Environment ```bash # 1. Fork and clone the repository git clone https://github.com/your-username/alabama-auction-watcher.git cd alabama-auction-watcher # 2. Create virtual environment python -m venv dev-env source dev-env/bin/activate # Linux/macOS # or dev-env\Scripts\activate # Windows # 3. Install development dependencies pip install -r requirements.txt pip install -r requirements-dev.txt # Development tools (if exists) # 4. Verify installation python scripts/parser.py --list-counties python -m pytest tests/ -v # Run tests (if available) ``` ## Architecture Overview ### System Components ``` Alabama Auction Watcher Architecture ===================================== ┌─────────────────────────────────────────────────────────────┐ │ Presentation Layer │ ├─────────────────────────────────────────────────────────────┤ │ streamlit_app/ │ │ ├── app.py (Main dashboard) │ │ └── components/ (Reusable UI components) │ └─────────────────────────────────────────────────────────────┘ │ ┌─────────────────────────────────────────────────────────────┐ │ Business Logic Layer │ ├─────────────────────────────────────────────────────────────┤ │ scripts/ │ │ ├── parser.py (Main orchestrator) │ │ ├── scraper.py (Web scraping engine) │ │ ├── utils.py (Data processing utilities) │ │ └── exceptions.py (Custom error handling) │ └─────────────────────────────────────────────────────────────┘ │ ┌─────────────────────────────────────────────────────────────┐ │ Configuration Layer │ ├─────────────────────────────────────────────────────────────┤ │ config/ │ │ ├── settings.py (Application configuration) │ │ └── logging_config.py (Logging setup) │ └─────────────────────────────────────────────────────────────┘ │ ┌─────────────────────────────────────────────────────────────┐ │ Data Layer │ ├─────────────────────────────────────────────────────────────┤ │ data/ │ │ ├── raw/ (Scraped CSV files) │ │ └── processed/ (Filtered watchlists) │ └─────────────────────────────────────────────────────────────┘ ``` ### Core Data Flow ``` 1. User Input → 2. County Validation → 3. Web Scraping → 4. Data Processing → 5. Output │ │ │ │ │ CLI Args validate_county() scrape_county() filter/rank CSV/Dashboard │ │ │ │ │ Config ALABAMA_COUNTY_ ADOR Website Investment Streamlit Files CODES Scoring Interface ``` ### Module Responsibilities | Module | Purpose | Key Functions | Dependencies | |--------|---------|---------------|--------------| | `parser.py` | Main orchestrator and CLI | `process_scraped_data()`, `main()` | scraper, utils, config | | `scraper.py` | Web scraping engine | `scrape_county_data()`, `validate_county_code()` | requests, BeautifulSoup | | `utils.py` | Data processing utilities | `parse_acreage_from_description()`, `calculate_water_score()` | pandas, numpy | | `exceptions.py` | Error handling | Custom exception classes | None (base module) | | `logging_config.py` | Structured logging | `setup_logging()`, `get_logger()` | logging | ## Development Workflow ### Branch Strategy ```bash main # Production-ready code ├── develop # Integration branch (if using) ├── feature/xyz # New features ├── bugfix/abc # Bug fixes └── hotfix/emergency # Critical production fixes ``` ### Feature Development Process #### 1. Planning Phase ```bash # Create GitHub issue first # Title: "Add batch county processing feature" # Include: requirements, acceptance criteria, technical approach ``` #### 2. Development Phase ```bash # Create feature branch git checkout -b feature/batch-county-processing # Follow TDD approach (recommended) # 1. Write tests first touch tests/test_batch_processing.py # 2. Implement feature # 3. Ensure tests pass python -m pytest tests/test_batch_processing.py -v # 4. Test integration python scripts/parser.py --batch-counties "Baldwin,Mobile" --max-pages 2 ``` #### 3. Code Quality Checks ```bash # Run full test suite python -m pytest tests/ -v # Check code style (if using) flake8 scripts/ config/ black --check scripts/ config/ # Type checking (if using) mypy scripts/ config/ ``` #### 4. Documentation ```bash # Update relevant documentation # - README.md (if user-facing changes) # - API_REFERENCE.md (if new functions) # - ROADMAP.md (if feature was planned) # Add comprehensive docstrings # Follow Google or NumPy docstring style ``` #### 5. Pull Request ```bash # Push feature branch git push origin feature/batch-county-processing # Create PR with template: # - Clear description of changes # - Test results and validation # - Breaking changes (if any) # - Documentation updates ``` ## Code Style Guidelines ### Python Conventions ```python # Follow PEP 8 with these specifics: # 1. Line length: 100 characters (not 79) def long_function_name( parameter_one: str, parameter_two: int, parameter_three: Optional[float] = None ) -> Dict[str, Any]: """Function with many parameters.""" pass # 2. Import organization import os import sys from pathlib import Path from typing import Dict, List, Optional import pandas as pd import requests from bs4 import BeautifulSoup from config.settings import MAX_PRICE from scripts.utils import normalize_price # 3. Type hints for all functions def process_county_data( county_code: str, max_pages: int = 10, save_output: bool = True ) -> pd.DataFrame: """Process county data with type safety.""" pass # 4. Error handling patterns try: result = risky_operation() except SpecificError as e: logger.error(f"Operation failed: {e}") raise CustomError(f"Failed to process: {e}") from e ``` ### Documentation Standards ```python def calculate_investment_score( price: float, acreage: float, water_score: float, assessed_value: Optional[float] = None ) -> float: """ Calculate composite investment score for a property. The investment score combines multiple factors to rank properties by their potential investment value. Higher scores indicate better investment opportunities. Args: price: Property purchase price in dollars acreage: Property size in acres water_score: Water feature score (0.0 to 10.0) assessed_value: County assessed value, if available Returns: Investment score between 0.0 and 100.0 Raises: ValueError: If price or acreage is negative Example: >>> calculate_investment_score(5000.0, 2.5, 3.0, 8000.0) 85.3 Note: Score weights are configurable in config/settings.py """ # Implementation here pass ``` ### Logging Standards ```python from config.logging_config import get_logger logger = get_logger(__name__) # Use structured logging def scrape_county(county: str) -> pd.DataFrame: logger.info(f"Starting scrape for county: {county}") try: # Implementation logger.info(f"Successfully scraped {len(df)} records") return df except Exception as e: logger.error(f"Scraping failed for {county}: {e}") raise ``` ## Testing Guidelines ### Test Structure ``` tests/ ├── __init__.py ├── conftest.py # Pytest configuration and fixtures ├── test_scraper.py # Web scraping tests ├── test_utils.py # Utility function tests ├── test_parser.py # Main parser tests ├── test_integration.py # End-to-end tests └── fixtures/ ├── sample_data.csv # Test data files └── mock_responses.html # Mock HTML responses ``` ### Testing Best Practices ```python import pytest import pandas as pd from unittest.mock import Mock, patch from scripts.scraper import validate_county_code, scrape_county_data from scripts.exceptions import CountyValidationError class TestCountyValidation: """Test county code validation functionality.""" def test_valid_county_code(self): """Test that valid county codes are accepted.""" assert validate_county_code('05') == '05' assert validate_county_code('5') == '05' # Zero padding def test_valid_county_name(self): """Test that valid county names are converted to codes.""" assert validate_county_code('Baldwin') == '05' assert validate_county_code('baldwin') == '05' # Case insensitive def test_invalid_county_raises_error(self): """Test that invalid counties raise appropriate errors.""" with pytest.raises(CountyValidationError): validate_county_code('InvalidCounty') @patch('scripts.scraper.requests.get') def test_scraping_with_mock(self, mock_get): """Test scraping with mocked HTTP responses.""" # Load fixture data with open('tests/fixtures/mock_responses.html', 'r') as f: mock_html = f.read() mock_response = Mock() mock_response.content = mock_html mock_response.status_code = 200 mock_get.return_value = mock_response result = scrape_county_data('05', max_pages=1) assert isinstance(result, pd.DataFrame) assert len(result) > 0 ``` ### Integration Testing ```python def test_end_to_end_workflow(): """Test complete workflow from scraping to output.""" # Use a county known to have data county = 'Baldwin' # Test scraping df = scrape_county_data(county, max_pages=1) assert len(df) > 0 # Test processing processed = process_property_data(df) assert 'investment_score' in processed.columns # Test filtering filtered = apply_filters(processed, min_acres=1.0, max_price=20000.0) assert len(filtered) <= len(processed) ``` ## Performance Guidelines ### Optimization Principles 1. **Memory Efficiency**: Use streaming for large datasets 2. **Network Courtesy**: Respect rate limits for web scraping 3. **Caching**: Cache expensive operations when appropriate 4. **Profiling**: Use cProfile for performance bottlenecks ### Performance Testing ```python import time from config.logging_config import log_performance def benchmark_scraping(): """Benchmark scraping performance.""" start_time = time.time() df = scrape_county_data('Baldwin', max_pages=1) duration = time.time() - start_time rate = len(df) / duration if duration > 0 else 0 log_performance( logger, "county_scraping", duration, records_processed=len(df) ) # Assert performance targets assert rate > 50.0 # Records per second assert duration < 30.0 # Max 30 seconds ``` ## Release Process ### Version Numbering Follow Semantic Versioning (SemVer): - **MAJOR.MINOR.PATCH** (e.g., 1.2.3) - **MAJOR**: Breaking changes - **MINOR**: New features (backward compatible) - **PATCH**: Bug fixes (backward compatible) ### Release Checklist ```bash # 1. Update version numbers echo "1.2.0" > VERSION # Update __version__ in relevant files # 2. Update CHANGELOG.md # Document all changes since last release # 3. Run comprehensive tests python -m pytest tests/ -v python scripts/parser.py --scrape-county Baldwin --max-pages 1 # 4. Update documentation # Ensure all new features are documented # 5. Create release commit git add . git commit -m "chore: prepare release v1.2.0" # 6. Tag release git tag -a v1.2.0 -m "Release v1.2.0: Add batch processing and improved logging" # 7. Push to repository git push origin main --tags ``` ## Community Guidelines ### Code of Conduct - **Be Respectful**: Treat all contributors with respect - **Be Constructive**: Provide helpful feedback and suggestions - **Be Inclusive**: Welcome contributors of all skill levels - **Be Patient**: Remember that everyone is learning ### Communication Channels - **GitHub Issues**: Bug reports and feature requests - **GitHub Discussions**: General questions and ideas - **Pull Requests**: Code contributions and reviews - **Discord/Slack**: Real-time community chat (if available) ### Contribution Types We welcome various types of contributions: #### **Bug Reports** - Use the bug report template - Include steps to reproduce - Provide system information - Include logs and error messages #### **Feature Requests** - Check existing issues first - Describe the problem you're solving - Propose implementation approach - Consider impact on existing users #### **Documentation** - Fix typos and improve clarity - Add examples and tutorials - Translate documentation - Improve code comments #### **Testing** - Add test cases for existing features - Improve test coverage - Add integration tests - Performance benchmarks #### **Code** - Follow the development workflow above - Include tests with your changes - Update documentation as needed - Follow code style guidelines ## Getting Help ### For New Contributors 1. **Start Small**: Look for issues labeled "good first issue" 2. **Ask Questions**: Use GitHub Discussions for help 3. **Read Documentation**: Review all .md files in the repository 4. **Join Community**: Participate in discussions and reviews ### For Experienced Developers 1. **Review Code**: Help review pull requests 2. **Mentor Others**: Guide new contributors 3. **Architectural Decisions**: Participate in design discussions 4. **Performance**: Help optimize and scale the system ### Common Development Issues See TROUBLESHOOTING.md for: - Environment setup problems - Dependency conflicts - Web scraping issues - Performance problems ## Additional Resources ### External Documentation - [Pandas Documentation](https://pandas.pydata.org/docs/) - [Requests Documentation](https://docs.python-requests.org/) - [BeautifulSoup Documentation](https://www.crummy.com/software/BeautifulSoup/bs4/doc/) - [Streamlit Documentation](https://docs.streamlit.io/) ### Project-Specific Guides - [DEPLOYMENT.md](DEPLOYMENT.md): Production deployment - [TROUBLESHOOTING.md](TROUBLESHOOTING.md): Common issues - [ROADMAP.md](ROADMAP.md): Future development plans - [API_REFERENCE.md](API_REFERENCE.md): Function documentation ### Learning Resources - [Web Scraping Best Practices](https://blog.apify.com/web-scraping-best-practices/) - [Data Processing with Pandas](https://pandas.pydata.org/pandas-docs/stable/user_guide/) - [Python Testing with Pytest](https://docs.pytest.org/en/stable/) --- ## Recognition Contributors will be recognized in: - **CONTRIBUTORS.md**: List of all contributors - **Release Notes**: Major contributors for each release - **GitHub Stars**: Pin important contributions - **Social Media**: Highlight significant contributions Thank you for contributing to Alabama Auction Watcher! Together, we're building the best tool for property investment analysis. --- **Maintainers**: Core Development Team **Last Updated**: September 2025 **Questions?**: Create a GitHub Discussion or Issue